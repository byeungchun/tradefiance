"""Convert IMF DOT series to Mongo DB format

Usage:
    {self_filename} <source_dir> <mongodb_database> <mongodb_collection> [options]

source_dir: path of source directory containing BIS series generated by download.py script
mongodb_database: 
mongodb_collection: 

Options:
    --debug                                     show debug output
    --only <datasets_codes_or_filenames>        only convert given dataset_code or filenames
                                                (ex: "--only CBS,DSRP", "--only full_WEBSTATS_DSR_DATAFLOW_csv.csv")
"""

import os
import re
import sys
import logging
import ujson as json
import pandas as pd
from datetime import datetime
from pymongo import MongoClient
from docopt import docopt
from collections import defaultdict
from toolz import merge

DATASET_CODES_AND_NAMES = {
  'DOT_timeSeries.csv': ('DOT', 'directions of trade statistics'),
}

JSONL_DATASETS = ['DOT']

log = logging.getLogger(__name__)

def main():
  global log
  global args

  args = docopt(__doc__.format(self_filename=os.path.basename(__file__)))
  source_dir = args['<source_dir>']
  assert os.path.exists(source_dir)
  dbname = args['<mongodb_database>']
  dbcollection = args['<mongodb_collection>']
  debug_mode = args['--debug']
  logging.basicConfig(level=(logging.DEBUG if debug_mode else logging.INFO), format='%(message)s')

  mgclient = MongoClient(unicode_decode_error_handler = 'ignore')
  mgdb = mgclient[dbname]
  mgcl_dot = mgdb[dbcollection]

  for csv_filename in [f for f in os.listdir(source_dir) if f.endswith('.csv')]:
    csv_filepath = os.path.join(source_dir, csv_filename)
    dot_parser = parse_dot_file(csv_filepath)
    dataset_json = next(dot_parser)
    for series_dict in dot_parser:
      mgcl_dot.insert_one(series_dict)
    log.info('Parsing({}) finished'.format(csv_filepath))

def parse_dot_file(dotfile):
  is_metadata = True
  dot_regexp = re.compile('(?:^|,)(?=[^"]|(")?)"?((?(1)[^"]*|[^,"]*))"?(?=,|$)')
  with open(dotfile, encoding='utf-8-sig') as dot_file:
    while True:
      line = dot_file.readline().strip()
      if line == '':
        break
      line = line[:-1] if len(line) > 0 and line[-1] == ',' else line
      elements = list(map(lambda x: x[1].strip(), dot_regexp.findall(line)))
      if is_metadata:
        assert 'Attribute' in elements
        nb_dimensions = elements.index('Attribute')
        dimensions = [cell for cell in elements[:nb_dimensions]]
        time_periods_labels = elements[nb_dimensions+1:]
        is_metadata = False
        yield {
          'dimensions': dimensions,
          'time_periods': time_periods_labels
        }
      else:
        if elements[nb_dimensions] == 'Status':
          continue
        dim_values = (dict(zip(dimensions, [cell for cell in elements[:nb_dimensions]])))
        observations = [
          (period, value)
          for period, value in zip(time_periods_labels, elements[nb_dimensions+1:]) if value != ''
        ]

        # Sort frequency 
        if all(len(x[0]) == 4 for x in observations): # Annual
          dim_values['freq'] = 'A'
          observations = [((pd.to_datetime(x[0]) + pd.offsets.YearEnd()).strftime('%Y%m%d'), float(x[1])) for x in observations]
        elif all('Q' in x[0] for x in observations): # Quarterly
          dim_values['freq'] = 'Q'
          observations = [((pd.to_datetime(x[0]) + pd.offsets.QuarterEnd()).strftime('%Y%m%d'), float(x[1])) for x in observations]
        elif all('M' in x[0] for x in observations): # Monthly
          dim_values['freq'] = 'M'
          observations = [((datetime.strptime(x[0].replace('M',''), '%Y%m') + pd.offsets.MonthEnd()).strftime('%Y%m%d'), float(x[1])) for x in observations]
        else:
          log.error('multiple frequencies in one row -> {}'.format('.'.join(line_dimensions_values_codes)))
          sys.exit(-1)
        dim_values.update({'timeseries': observations})

        yield dim_values

if __name__=="__main__":
  sys.exit(main())